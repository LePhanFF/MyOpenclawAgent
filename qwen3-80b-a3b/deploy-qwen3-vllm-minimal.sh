#!/bin/bash

# DGX Spark vLLM Deployment - Minimal FlashInfer Usage
# Attempt to run Qwen3 with minimal flashinfer features

set -e

echo "ğŸš€ DGX Spark vLLM Deployment - Minimal FlashInfer Mode"
echo "========================================================"

# 1. Cleanup existing containers
echo "ğŸ§¹ Cleaning up existing containers..."
docker stop vllm_qwen 2>/dev/null || true
docker rm vllm_qwen 2>/dev/null || true

# 2. Pull optimized DGX Spark image
echo "ğŸ“¦ Pulling DGX Spark optimized vLLM image..."
docker pull scitrera/dgx-spark-vllm:0.13.0-t4

# 3. Launch with minimal flashinfer usage
echo "ğŸ”¥ Launching vLLM server with minimal flashinfer..."
echo "ğŸ“Š Configuration:"
echo "   - Model: nvidia/Qwen3-Next-80B-A3B-Instruct-NVFP4"
echo "   - GPU Memory Utilization: 70%"
echo "   - Max Context Length: 32K tokens"
echo "   - KV Cache: auto (avoid FP8)"
echo "   - Port: 8356"
echo "   - Attention Backend: Triton (avoid FlashInfer)"

docker run -d \
  --name vllm_qwen \
  --gpus '"device=0"' \
  --shm-size=16g \
  -p 8356:8356 \
  -v "$HOME/.cache/huggingface:/root/.cache/huggingface" \
  scitrera/dgx-spark-vllm:0.13.0-t4 \
  vllm serve \
        "nvidia/Qwen3-Next-80B-A3B-Instruct-NVFP4" \
        --port 8356 \
        --max-model-len 32768 \
        --gpu-memory-utilization 0.70 \
        --attention-backend triton \
        --enforce-eager \
        --disable-frontend-multiprocessing \
        --trust-remote-code

# 4. Wait for server to be ready
echo "â³ Waiting for vLLM server to initialize..."
sleep 90

# 5. Check container status
echo "ğŸ“‹ Checking container status..."
docker ps | grep vllm_qwen

# 6. Test the endpoint
echo "ğŸ§ª Testing vLLM endpoint..."
if curl -s http://localhost:8356/v1/models > /dev/null; then
    echo "âœ… vLLM server is healthy and ready!"
    MODEL=$(curl -s http://localhost:8356/v1/models | jq -r '.data[0].id')
    echo "ğŸ¤– Loaded model: $MODEL"
    echo "ğŸŒ API Endpoint: http://localhost:8356/v1"
    
    # Test chat completion
    echo "ğŸ’¬ Testing chat completion..."
    RESPONSE=$(curl -s -X POST http://localhost:8356/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d "{
        \"model\": \"$MODEL\",
        \"messages\": [
          {\"role\": \"user\", \"content\": \"Say 'OpenClaw integration successful!'\"}
        ],
        \"max_tokens\": 15
      }" | jq -r '.choices[0].message.content')
    
    if [ -n "$RESPONSE" ] && [ "$RESPONSE" != "null" ]; then
        echo "âœ… Chat completion working: $RESPONSE"
        echo "ğŸ‰ NVFP4 model successfully deployed!"
    else
        echo "âŒ Chat completion failed"
        docker logs vllm_qwen --tail 20
    fi
else
    echo "âŒ Server not ready, checking logs..."
    docker logs vllm_qwen --tail 50
fi

echo "ğŸ”— Integration ready for OpenClaw"